# Deep Learning Replications

These are a mixture of partial replications of landmark papers in deep learning research and small investigative projects inspired by those papers. 

In many cases, the projects follow recommendations Jacob Hilton makes in his Deep Learning Curriculum, which is available athttps://github.com/jacobhilton/deep_learning_curriculum

All of them currently work with the MPS device in the latest pytorch release.

## WIP
\# Scaling Laws 

This is a test following Kaplan et. al.'s "Scaling Laws for Neural Language Models" and Hoffman et. al.'s "Training Compute-Optimal Large Language Models". Since I don't have access to enough compute to estimate the relation between loss, model size, and data size for language models, I've followed Hilton's recommendation of using a small conv net with the MNIST data set. 

\#6 Reinforcement Learning

## To-Do
\#3 Training at Scale

\#4 Optimization

\#5 Modeling Objectives

\#7 Alignment

\#8 Interpretability

\#9 Adversarial Training

## Done for now
\#1 Transformers
